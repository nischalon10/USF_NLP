{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# STD LSTM (tensorflow)"
      ],
      "metadata": {
        "id": "pEgrppHtlxOK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# Define hyperparameters\n",
        "HIDDEN_SIZE = 50\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 0.001\n",
        "SEQ_LENGTH = 20\n",
        "VOCAB_SIZE = 10  # Digits 0-9\n",
        "DELIMITER = \"=\"\n",
        "\n",
        "class LSTMCell(tf.keras.layers.Layer):\n",
        "    def __init__(self, units):\n",
        "        super(LSTMCell, self).__init__()\n",
        "        self.units = units\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        input_dim = input_shape[-1]\n",
        "        self.W_input = self.add_weight(shape=(input_dim, self.units), initializer='random_normal', trainable=True)\n",
        "        self.U_input = self.add_weight(shape=(self.units, self.units), initializer='random_normal', trainable=True)\n",
        "        self.b_input = self.add_weight(shape=(self.units,), initializer='zeros', trainable=True)\n",
        "\n",
        "        self.W_forget = self.add_weight(shape=(input_dim, self.units), initializer='random_normal', trainable=True)\n",
        "        self.U_forget = self.add_weight(shape=(self.units, self.units), initializer='random_normal', trainable=True)\n",
        "        self.b_forget = self.add_weight(shape=(self.units,), initializer='zeros', trainable=True)\n",
        "\n",
        "        self.W_output = self.add_weight(shape=(input_dim, self.units), initializer='random_normal', trainable=True)\n",
        "        self.U_output = self.add_weight(shape=(self.units, self.units), initializer='random_normal', trainable=True)\n",
        "        self.b_output = self.add_weight(shape=(self.units,), initializer='zeros', trainable=True)\n",
        "\n",
        "        self.W_c_compliment = self.add_weight(shape=(input_dim, self.units), initializer='random_normal', trainable=True)\n",
        "        self.U_c_compliment = self.add_weight(shape=(self.units, self.units), initializer='random_normal', trainable=True)\n",
        "        self.b_c_compliment = self.add_weight(shape=(self.units,), initializer='zeros', trainable=True)\n",
        "\n",
        "    def call(self, x, states):\n",
        "        hidden_prev, cell_prev = states\n",
        "\n",
        "        input_gate = tf.sigmoid(tf.matmul(x, self.W_input) + tf.matmul(hidden_prev, self.U_input) + self.b_input)\n",
        "        forget_gate = tf.sigmoid(tf.matmul(x, self.W_forget) + tf.matmul(hidden_prev, self.U_forget) + self.b_forget)\n",
        "        output_gate = tf.sigmoid(tf.matmul(x, self.W_output) + tf.matmul(hidden_prev, self.U_output) + self.b_output)\n",
        "        c_compliment = tf.tanh(tf.matmul(x, self.W_c_compliment) + tf.matmul(hidden_prev, self.U_c_compliment) + self.b_c_compliment)\n",
        "\n",
        "        cell_state = forget_gate * cell_prev + input_gate * c_compliment\n",
        "        hidden_state = output_gate * tf.tanh(cell_state)\n",
        "\n",
        "        return hidden_state, [hidden_state, cell_state]\n",
        "\n",
        "class LSTMModel(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, hidden_size):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size + 2, hidden_size)  # +2 for delimiter and padding\n",
        "        self.lstm = tf.keras.layers.LSTM(hidden_size, return_sequences=True, return_state=True)\n",
        "        self.dense = tf.keras.layers.Dense(vocab_size, activation='softmax')\n",
        "\n",
        "    def call(self, inputs, states=None):\n",
        "        x = self.embedding(inputs)\n",
        "        x, hidden_state, cell_state = self.lstm(x, initial_state=states)\n",
        "        output = self.dense(x)\n",
        "        return output, [hidden_state, cell_state]\n",
        "\n",
        "# Data Generation\n",
        "def generate_data(num_samples=10000):\n",
        "    X_data, Y_data = [], []\n",
        "    for _ in range(num_samples):\n",
        "        seq_length = np.random.randint(1, SEQ_LENGTH + 1)\n",
        "        sequence = np.random.randint(0, VOCAB_SIZE, size=(seq_length,)).tolist()\n",
        "        sequence.append(VOCAB_SIZE)  # Append delimiter\n",
        "        target = sequence[:-1]  # Expected output (sequence before delimiter)\n",
        "        X_data.append(sequence)\n",
        "        Y_data.append(target)\n",
        "    return np.array(X_data, dtype=np.int32), np.array(Y_data, dtype=np.int32)\n",
        "\n",
        "# Prepare dataset\n",
        "X, Y = generate_data()\n",
        "split_idx = int(0.7 * len(X))\n",
        "X_train, Y_train = X[:split_idx], Y[:split_idx]\n",
        "X_val, Y_val = X[split_idx:], Y[split_idx:]\n",
        "\n",
        "# Model Training\n",
        "model = LSTMModel(VOCAB_SIZE, HIDDEN_SIZE)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "\n",
        "model.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy'])\n",
        "history = model.fit(X_train, Y_train, validation_data=(X_val, Y_val), batch_size=BATCH_SIZE, epochs=10, verbose=1)\n",
        "\n",
        "# Print training statistics\n",
        "for epoch in range(len(history.history['loss'])):\n",
        "    print(f\"Epoch {epoch+1}/{len(history.history['loss'])} - Loss: {history.history['loss'][epoch]:.4f}, Accuracy: {history.history['accuracy'][epoch]:.4f}, Val Loss: {history.history['val_loss'][epoch]:.4f}, Val Accuracy: {history.history['val_accuracy'][epoch]:.4f}\")\n"
      ],
      "metadata": {
        "id": "4Xsj1xXUl15i",
        "outputId": "4d76ada0-359f-45c2-cfd1-4924340635f3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss after one training step: 2.393489122390747\n",
            "\n",
            "Test input sequence: 3541=\n",
            "Predicted output: 83898\n"
          ]
        }
      ]
    }
  ]
}
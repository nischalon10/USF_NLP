{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# STD LSTM (tensorflow)"
      ],
      "metadata": {
        "id": "pEgrppHtlxOK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# Hyperparameters\n",
        "num_samples = 10000   # total number of examples\n",
        "T = 100                # length of the sequence to copy (you can experiment with {100, 200, 500, 1000})\n",
        "vocab_size = 10       # tokens 1..10 (e.g., letters a-j)\n",
        "hidden_size = 128\n",
        "batch_size = 128\n",
        "epochs = 20\n",
        "\n",
        "# Define special tokens:\n",
        "# 0 will be used as the blank token.\n",
        "# The delimiter token will be vocab_size + 1.\n",
        "delimiter_token = vocab_size + 1\n",
        "\n",
        "def generate_copy_data(num_samples, T, vocab_size):\n",
        "    \"\"\"\n",
        "    Generate data for the copy task.\n",
        "    For each sample:\n",
        "      - Generate a random sequence of T tokens (from 1 to vocab_size).\n",
        "      - Create an input sequence of length 2*T + 1:\n",
        "           [random sequence] + [delimiter] + [T blanks (0)]\n",
        "      - Create a target sequence of the same length:\n",
        "           [T+1 blanks] + [the original random sequence]\n",
        "    The model is only evaluated on its output in the last T time steps.\n",
        "    \"\"\"\n",
        "    seq_length = 2 * T + 1\n",
        "    X = np.zeros((num_samples, seq_length), dtype=np.int32)\n",
        "    Y = np.zeros((num_samples, seq_length), dtype=np.int32)\n",
        "\n",
        "    for i in range(num_samples):\n",
        "        # Generate a random sequence (tokens from 1 to vocab_size)\n",
        "        random_seq = np.random.randint(1, vocab_size + 1, size=T)\n",
        "        # Build input: first T tokens are the random sequence\n",
        "        X[i, :T] = random_seq\n",
        "        # Then one delimiter token\n",
        "        X[i, T] = delimiter_token\n",
        "        # Followed by T blank tokens (0)\n",
        "        X[i, T+1:] = 0\n",
        "\n",
        "        # Build target: first T+1 positions are blanks (0)\n",
        "        Y[i, :T+1] = 0\n",
        "        # Last T positions are the original sequence\n",
        "        Y[i, T+1:] = random_seq\n",
        "    return X, Y\n",
        "\n",
        "# Generate data\n",
        "X, Y = generate_copy_data(num_samples, T, vocab_size)\n",
        "\n",
        "# Split data into training (70%) and validation (30%)\n",
        "split_index = int(0.7 * num_samples)\n",
        "X_train, Y_train = X[:split_index], Y[:split_index]\n",
        "X_val, Y_val = X[split_index:], Y[split_index:]\n",
        "\n",
        "# Create tf.data.Dataset objects\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, Y_train))\n",
        "train_dataset = train_dataset.shuffle(1000).batch(batch_size)\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((X_val, Y_val)).batch(batch_size)\n",
        "\n",
        "# Define the LSTM model.\n",
        "# Note: This uses the built-in Keras LSTM layer.\n",
        "class LSTMModel(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, hidden_size):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        # The embedding layer input dimension is vocab_size + 2 because\n",
        "        # we use 0 for blank and (vocab_size+1) for the delimiter.\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size + 2, hidden_size)\n",
        "        # Using Keras' LSTM; if you want to use your custom LSTMCell, wrap it with tf.keras.layers.RNN.\n",
        "        self.lstm = tf.keras.layers.LSTM(hidden_size, return_sequences=True, return_state=True)\n",
        "        # The dense layer maps to (vocab_size + 1) outputs.\n",
        "        # We exclude the delimiter from the outputs as the target tokens are in the range [0, vocab_size]\n",
        "        self.dense = tf.keras.layers.Dense(vocab_size + 1, activation='softmax')\n",
        "\n",
        "    def call(self, inputs, states=None, training=False):\n",
        "        x = self.embedding(inputs)\n",
        "        if states is None:\n",
        "            x, h, c = self.lstm(x)\n",
        "        else:\n",
        "            x, h, c = self.lstm(x, initial_state=states)\n",
        "        output = self.dense(x)\n",
        "        return output\n",
        "\n",
        "# Instantiate the model\n",
        "model = LSTMModel(vocab_size, hidden_size)\n",
        "\n",
        "# Compile the model using sparse categorical crossentropy.\n",
        "# The model output shape is (batch, time, vocab_size+1) and targets are integers.\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(train_dataset, epochs=epochs, validation_data=val_dataset)\n",
        "\n",
        "# Plot training and validation loss and accuracy\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Plot Loss\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['loss'], marker='o', label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], marker='o', label='Validation Loss')\n",
        "plt.title('Loss over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "# Plot Accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['accuracy'], marker='o', label='Train Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], marker='o', label='Validation Accuracy')\n",
        "plt.title('Accuracy over Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Xsj1xXUl15i",
        "outputId": "cddb0e42-7527-4483-f415-16a0688fdfb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m55/55\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 796ms/step - accuracy: 0.5091 - loss: 1.4470 - val_accuracy: 0.5517 - val_loss: 1.1492\n",
            "Epoch 2/20\n",
            "\u001b[1m10/55\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 709ms/step - accuracy: 0.5515 - loss: 1.1491"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mapping for letters (a-j) to token integers and vice versa\n",
        "letter_to_token = {'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5, 'f': 6, 'g': 7, 'h': 8, 'i': 9, 'j': 10}\n",
        "token_to_letter = {v: k for k, v in letter_to_token.items()}\n",
        "\n",
        "# Define special tokens\n",
        "delimiter_token = 11  # As used in our training (vocab_size + 1)\n",
        "blank_token = 0       # Blank token\n",
        "\n",
        "# Example custom input: The sequence (letters) followed by a delimiter \"=\".\n",
        "# For instance, \"ababdceijhg=\" represents the sequence to copy.\n",
        "custom_input = \"ababdceijhg=\"\n",
        "\n",
        "# Ensure the input ends with the delimiter marker '='\n",
        "if not custom_input.endswith('='):\n",
        "    raise ValueError(\"Custom input must end with '=' to indicate the delimiter.\")\n",
        "\n",
        "# Remove the delimiter from the string to obtain the original sequence to be copied\n",
        "original_seq_str = custom_input[:-1]\n",
        "\n",
        "# Convert each letter in the sequence to its corresponding token\n",
        "input_seq_tokens = [letter_to_token[letter] for letter in original_seq_str]\n",
        "\n",
        "# For the copy task, the full input sequence for the model should be:\n",
        "# [original sequence tokens] + [delimiter token] + [T blank tokens],\n",
        "# where T is the length of the original sequence.\n",
        "T_custom = len(original_seq_str)\n",
        "model_input_tokens = input_seq_tokens + [delimiter_token] + [blank_token] * T_custom\n",
        "\n",
        "# Convert to a numpy array and add a batch dimension (shape: [1, sequence_length])\n",
        "import numpy as np\n",
        "model_input = np.array(model_input_tokens).reshape(1, -1)\n",
        "\n",
        "# Get predictions from the model.\n",
        "# The model output shape is (batch, time, vocab_size+1), and we take argmax over the last axis.\n",
        "predictions = model.predict(model_input)\n",
        "predicted_tokens = np.argmax(predictions, axis=-1)[0]\n",
        "\n",
        "# The model is only expected to produce the copied sequence in the last T time steps.\n",
        "predicted_seq_tokens = predicted_tokens[T_custom+1:]\n",
        "\n",
        "# Convert predicted tokens back to letters\n",
        "predicted_seq_letters = ''.join(token_to_letter.get(token, '') for token in predicted_seq_tokens)\n",
        "\n",
        "print(\"Custom Input:\", custom_input)\n",
        "print(\"Predicted Output:\", predicted_seq_letters)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "szsKh89G8o9o",
        "outputId": "d6f6918a-a539-415d-d029-bc3f76b3c592"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 198ms/step\n",
            "Custom Input: ababdceijhg=\n",
            "Predicted Output: gggddddjjjj\n"
          ]
        }
      ]
    }
  ]
}
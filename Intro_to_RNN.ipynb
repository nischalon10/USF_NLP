{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setup of Main\n"
      ],
      "metadata": {
        "id": "eaNvSVThvGXp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "# Data generation for the copy task.\n",
        "def generate_copy_data(num_samples, T, vocab_size):\n",
        "    \"\"\"\n",
        "    For each sample:\n",
        "      - Generate a random sequence of length T (tokens from 1 to vocab_size).\n",
        "      - Build input: [sequence] + [delimiter token] + [T blank tokens (0)]\n",
        "      - Build target: [T+1 blanks] + [the original sequence]\n",
        "    \"\"\"\n",
        "    seq_length = 2 * T + 1\n",
        "    X = np.zeros((num_samples, seq_length), dtype=np.int32)\n",
        "    Y = np.zeros((num_samples, seq_length), dtype=np.int32)\n",
        "    delimiter_token = vocab_size + 1  # special delimiter token\n",
        "    for i in range(num_samples):\n",
        "        # Random sequence of T tokens (range: 1 to vocab_size)\n",
        "        random_seq = np.random.randint(1, vocab_size + 1, size=T)\n",
        "        # Input: first T tokens are the sequence, then a delimiter, then T blanks (0)\n",
        "        X[i, :T] = random_seq\n",
        "        X[i, T] = delimiter_token\n",
        "        X[i, T+1:] = 0\n",
        "        # Target: first T+1 positions are blanks (0), last T positions are the sequence\n",
        "        Y[i, :T+1] = 0\n",
        "        Y[i, T+1:] = random_seq\n",
        "    return X, Y\n",
        "\n",
        "# Hyperparameters\n",
        "num_samples = 10000    # total examples\n",
        "T = 10                # sequence length to copy (experiment with larger values as needed)\n",
        "vocab_size = 10       # tokens 1..10 (e.g., representing letters a-j)\n",
        "hidden_size = 128\n",
        "batch_size = 128\n",
        "epochs = 20\n",
        "\n",
        "delimiter_token = vocab_size + 1  # used for both data generation and model processing\n",
        "blank_token = 0\n",
        "\n",
        "# Generate the copy task data.\n",
        "X, Y = generate_copy_data(num_samples, T, vocab_size)\n",
        "\n",
        "# Split into training (70%) and validation (30%).\n",
        "split_index = int(0.7 * num_samples)\n",
        "X_train, Y_train = X[:split_index], Y[:split_index]\n",
        "X_val, Y_val = X[split_index:], Y[split_index:]\n",
        "\n",
        "# Create tf.data.Dataset objects.\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, Y_train)).shuffle(1000).batch(batch_size)\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((X_val, Y_val)).batch(batch_size)\n",
        "\n"
      ],
      "metadata": {
        "id": "-zk937GFvM7s"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STD LSTM (tensorflow)"
      ],
      "metadata": {
        "id": "pEgrppHtlxOK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class STDLSTMCell(tf.keras.layers.Layer):\n",
        "    def __init__(self, units):\n",
        "        super(STDLSTMCell, self).__init__()\n",
        "        self.units = units\n",
        "        # Define the state size: hidden state and cell state.\n",
        "        self.state_size = [self.units, self.units]\n",
        "        self.output_size = self.units\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        input_dim = input_shape[-1]\n",
        "        # Input gate weights and bias\n",
        "        self.W_i = self.add_weight(shape=(input_dim, self.units), initializer='random_normal', trainable=True)\n",
        "        self.U_i = self.add_weight(shape=(self.units, self.units), initializer='random_normal', trainable=True)\n",
        "        self.b_i = self.add_weight(shape=(self.units,), initializer='zeros', trainable=True)\n",
        "        # Forget gate weights and bias\n",
        "        self.W_f = self.add_weight(shape=(input_dim, self.units), initializer='random_normal', trainable=True)\n",
        "        self.U_f = self.add_weight(shape=(self.units, self.units), initializer='random_normal', trainable=True)\n",
        "        self.b_f = self.add_weight(shape=(self.units,), initializer='zeros', trainable=True)\n",
        "        # Output gate weights and bias\n",
        "        self.W_o = self.add_weight(shape=(input_dim, self.units), initializer='random_normal', trainable=True)\n",
        "        self.U_o = self.add_weight(shape=(self.units, self.units), initializer='random_normal', trainable=True)\n",
        "        self.b_o = self.add_weight(shape=(self.units,), initializer='zeros', trainable=True)\n",
        "        # Candidate cell state weights and bias\n",
        "        self.W_c = self.add_weight(shape=(input_dim, self.units), initializer='random_normal', trainable=True)\n",
        "        self.U_c = self.add_weight(shape=(self.units, self.units), initializer='random_normal', trainable=True)\n",
        "        self.b_c = self.add_weight(shape=(self.units,), initializer='zeros', trainable=True)\n",
        "\n",
        "    def call(self, x, states):\n",
        "        h_prev, c_prev = states\n",
        "\n",
        "        # Input gate: i_t = sigmoid(W_i * x_t + U_i * h_(t-1) + b_i)\n",
        "        i = tf.sigmoid(tf.matmul(x, self.W_i) + tf.matmul(h_prev, self.U_i) + self.b_i)\n",
        "        # Forget gate: f_t = sigmoid(W_f * x_t + U_f * h_(t-1) + b_f)\n",
        "        f = tf.sigmoid(tf.matmul(x, self.W_f) + tf.matmul(h_prev, self.U_f) + self.b_f)\n",
        "        # Output gate: o_t = sigmoid(W_o * x_t + U_o * h_(t-1) + b_o)\n",
        "        o = tf.sigmoid(tf.matmul(x, self.W_o) + tf.matmul(h_prev, self.U_o) + self.b_o)\n",
        "        # Candidate cell state: c̃_t = tanh(W_c * x_t + U_c * h_(t-1) + b_c)\n",
        "        c_hat = tf.tanh(tf.matmul(x, self.W_c) + tf.matmul(h_prev, self.U_c) + self.b_c)\n",
        "        # New cell state: c_t = f_t ⊙ c_(t-1) + i_t ⊙ c̃_t\n",
        "        c = f * c_prev + i * c_hat\n",
        "        # New hidden state: h_t = o_t ⊙ tanh(c_t)\n",
        "        h = o * tf.tanh(c)\n",
        "\n",
        "        return h, [h, c]\n",
        "\n",
        "# Custom LSTM model that uses the above custom cell.\n",
        "class STDLSTMModel(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, hidden_size):\n",
        "        super(STDLSTMModel, self).__init__()\n",
        "        # The embedding layer maps input tokens to vectors.\n",
        "        # We use vocab_size+2 because token 0 is blank and token (vocab_size+1) is the delimiter.\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size + 2, hidden_size)\n",
        "        # Wrap the custom LSTM cell with tf.keras.layers.RNN.\n",
        "        self.rnn = tf.keras.layers.RNN(STDLSTMCell(hidden_size), return_sequences=True, return_state=True)\n",
        "        # Dense layer to project RNN outputs to (vocab_size+1) probabilities.\n",
        "        self.dense = tf.keras.layers.Dense(vocab_size + 1, activation='softmax')\n",
        "\n",
        "    def call(self, inputs, initial_state=None, training=False):\n",
        "        x = self.embedding(inputs)\n",
        "        if initial_state is None:\n",
        "            rnn_out, h, c = self.rnn(x)\n",
        "        else:\n",
        "            rnn_out, h, c = self.rnn(x, initial_state=initial_state)\n",
        "        output = self.dense(rnn_out)\n",
        "        return output"
      ],
      "metadata": {
        "id": "4Xsj1xXUl15i"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the Model"
      ],
      "metadata": {
        "id": "GAvRNL9MvgHO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate and compile the custom LSTM model.\n",
        "custom_model = STDLSTMModel(vocab_size, hidden_size)\n",
        "custom_model.compile(optimizer='adam',\n",
        "                     loss='sparse_categorical_crossentropy',\n",
        "                     metrics=['accuracy'])\n",
        "\n",
        "# Train the model.\n",
        "history = custom_model.fit(train_dataset, epochs=epochs, validation_data=val_dataset)\n",
        "\n",
        "# Plot training loss and accuracy.\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['loss'], marker='o', label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], marker='o', label='Validation Loss')\n",
        "plt.title('Loss vs. Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['accuracy'], marker='o', label='Train Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], marker='o', label='Validation Accuracy')\n",
        "plt.title('Accuracy vs. Epochs')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "fGUFd7jMvjO3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MUL LSTM (tensorflow)"
      ],
      "metadata": {
        "id": "g2R40not0xkE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLSTMCell(tf.keras.layers.Layer):\n",
        "    def __init__(self, units):\n",
        "        super(MLSTMCell, self).__init__()\n",
        "        self.units = units\n",
        "        # State size: hidden state (h) and cell state (c).\n",
        "        self.state_size = [self.units, self.units]\n",
        "        self.output_size = self.units\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        input_dim = input_shape[-1]\n",
        "        # -- Multiplicative term parameters: m_t = W_m x_t + U_m h_(t-1) + b_m --\n",
        "        # Produces a vector of size [batch_size, input_dim], which is then\n",
        "        # elementwise-multiplied with x_t to form x̃_t.\n",
        "        self.W_m = self.add_weight( shape=(input_dim, input_dim), initializer=\"random_normal\", trainable=True, name=\"W_m\",)\n",
        "        self.U_m = self.add_weight( shape=(self.units, input_dim), initializer=\"random_normal\", trainable=True, name=\"U_m\",)\n",
        "        self.b_m = self.add_weight( shape=(input_dim,), initializer=\"zeros\", trainable=True, name=\"b_m\")\n",
        "\n",
        "        # -- LSTM gate parameters (same shapes as standard LSTM) --\n",
        "        # Input gate\n",
        "        self.W_i = self.add_weight( shape=(input_dim, self.units), initializer=\"random_normal\", trainable=True, name=\"W_i\",)\n",
        "        self.U_i = self.add_weight( shape=(self.units, self.units), initializer=\"random_normal\", trainable=True, name=\"U_i\",)\n",
        "        self.b_i = self.add_weight( shape=(self.units,), initializer=\"zeros\", trainable=True, name=\"b_i\")\n",
        "\n",
        "        # Forget gate\n",
        "        self.W_f = self.add_weight( shape=(input_dim, self.units), initializer=\"random_normal\", trainable=True, name=\"W_f\",)\n",
        "        self.U_f = self.add_weight( shape=(self.units, self.units), initializer=\"random_normal\", trainable=True, name=\"U_f\",)\n",
        "        self.b_f = self.add_weight( shape=(self.units,), initializer=\"zeros\", trainable=True, name=\"b_f\")\n",
        "\n",
        "        # Output gate\n",
        "        self.W_o = self.add_weight( shape=(input_dim, self.units), initializer=\"random_normal\", trainable=True, name=\"W_o\",)\n",
        "        self.U_o = self.add_weight( shape=(self.units, self.units), initializer=\"random_normal\", trainable=True, name=\"U_o\",)\n",
        "        self.b_o = self.add_weight( shape=(self.units,), initializer=\"zeros\", trainable=True, name=\"b_o\")\n",
        "\n",
        "        # Candidate cell state\n",
        "        self.W_c = self.add_weight( shape=(input_dim, self.units), initializer=\"random_normal\", trainable=True, name=\"W_c\",)\n",
        "        self.U_c = self.add_weight( shape=(self.units, self.units), initializer=\"random_normal\", trainable=True, name=\"U_c\",)\n",
        "        self.b_c = self.add_weight( shape=(self.units,), initializer=\"zeros\", trainable=True, name=\"b_c\")\n",
        "\n",
        "    def call(self, x, states):\n",
        "        \"\"\"\n",
        "        x:      [batch_size, input_dim]\n",
        "        states: [h_prev, c_prev], each of shape [batch_size, units]\n",
        "        \"\"\"\n",
        "        h_prev, c_prev = states\n",
        "        #  m_t = W_m * x + U_m * h_prev + b_m\n",
        "        m_t = tf.matmul(x, self.W_m) + tf.matmul(h_prev, self.U_m) + self.b_m\n",
        "        #  x̃_t = m_t ⊙ x\n",
        "        x_tilde = m_t * x  # shape: [batch_size, input_dim]\n",
        "        # Input gate: i_t\n",
        "        i = tf.sigmoid(tf.matmul(x_tilde, self.W_i) + tf.matmul(h_prev, self.U_i) + self.b_i)\n",
        "        # Forget gate: f_t\n",
        "        f = tf.sigmoid(tf.matmul(x_tilde, self.W_f) + tf.matmul(h_prev, self.U_f) + self.b_f)\n",
        "        # Output gate: o_t\n",
        "        o = tf.sigmoid(tf.matmul(x_tilde, self.W_o) + tf.matmul(h_prev, self.U_o) + self.b_o)\n",
        "        # Candidate cell state: c̃_t\n",
        "        c_hat = tf.tanh(tf.matmul(x_tilde, self.W_c) + tf.matmul(h_prev, self.U_c) + self.b_c)\n",
        "        # New cell state: c_t\n",
        "        c = f * c_prev + i * c_hat\n",
        "        # New hidden state: h_t\n",
        "        h = o * tf.tanh(c)\n",
        "        return h, [h, c]\n",
        "\n",
        "class MLSTMModel(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, hidden_size):\n",
        "        super(MLSTMModel, self).__init__()\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size + 2, hidden_size)\n",
        "        self.rnn = tf.keras.layers.RNN(\n",
        "            MLSTMCell(hidden_size),\n",
        "            return_sequences=True,\n",
        "            return_state=True\n",
        "        )\n",
        "        # Dense layer to project RNN outputs to (vocab_size+1) probabilities.\n",
        "        self.dense = tf.keras.layers.Dense(vocab_size + 1, activation='softmax')\n",
        "\n",
        "    def call(self, inputs, initial_state=None, training=False):\n",
        "        x = self.embedding(inputs)\n",
        "        if initial_state is None:\n",
        "            rnn_out, h, c = self.rnn(x)  # automatically uses zeros if no init state\n",
        "        else:\n",
        "            rnn_out, h, c = self.rnn(x, initial_state=initial_state)\n",
        "        output = self.dense(rnn_out)\n",
        "        return output"
      ],
      "metadata": {
        "id": "gdpedyU301s6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STD GRU (tensorflow)"
      ],
      "metadata": {
        "id": "UcT_4qID3Vkx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class StandardGRUCell(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    A custom implementation of the standard GRU cell using TensorFlow,\n",
        "    following the equations:\n",
        "\n",
        "        z_t = sigma(W_z * x_t + U_z * h_{t-1} + b_z)\n",
        "        r_t = sigma(W_r * x_t + U_r * h_{t-1} + b_r)\n",
        "        h_tilde = tanh(W_h * x_t + U_h * (r_t * h_{t-1}) + b_h)\n",
        "        h_t = (1 - z_t) * h_{t-1} + z_t * h_tilde\n",
        "    \"\"\"\n",
        "    def __init__(self, units, **kwargs):\n",
        "        super(StandardGRUCell, self).__init__(**kwargs)\n",
        "        self.units = units\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # input_shape is (batch_size, input_dim)\n",
        "        input_dim = input_shape[-1]\n",
        "        self.Wz = self.add_weight( shape=(input_dim, self.units), initializer='glorot_uniform', name='Wz' )\n",
        "        self.Uz = self.add_weight( shape=(self.units, self.units), initializer='orthogonal', name='Uz' )\n",
        "        self.bz = self.add_weight( shape=(self.units,), initializer='zeros', name='bz' )\n",
        "        # Weights for reset gate r\n",
        "        self.Wr = self.add_weight( shape=(input_dim, self.units), initializer='glorot_uniform', name='Wr' )\n",
        "        self.Ur = self.add_weight( shape=(self.units, self.units), initializer='orthogonal', name='Ur' )\n",
        "        self.br = self.add_weight( shape=(self.units,), initializer='zeros', name='br' )\n",
        "        # Weights for candidate hidden state h_tilde\n",
        "        self.Wh = self.add_weight( shape=(input_dim, self.units), initializer='glorot_uniform', name='Wh' )\n",
        "        self.Uh = self.add_weight( shape=(self.units, self.units), initializer='orthogonal', name='Uh' )\n",
        "        self.bh = self.add_weight( shape=(self.units,), initializer='zeros', name='bh' )\n",
        "        super(StandardGRUCell, self).build(input_shape)\n",
        "\n",
        "    def call(self, inputs, states):\n",
        "        \"\"\"\n",
        "        inputs: (batch_size, input_dim)\n",
        "        states: list with a single element h_{t-1} of shape (batch_size, units)\n",
        "        \"\"\"\n",
        "        h_prev = states[0]\n",
        "        z_t = tf.sigmoid( tf.matmul(inputs, self.Wz) + tf.matmul(h_prev, self.Uz) + self.bz )\n",
        "        # Reset gate r_t\n",
        "        r_t = tf.sigmoid( tf.matmul(inputs, self.Wr) + tf.matmul(h_prev, self.Ur) + self.br )\n",
        "        # Candidate hidden state h_tilde\n",
        "        h_tilde = tf.tanh( tf.matmul(inputs, self.Wh) + tf.matmul(r_t * h_prev, self.Uh) + self.bh )\n",
        "        # Final new hidden state h_t\n",
        "        h_t = (1 - z_t) * h_prev + z_t * h_tilde\n",
        "        return h_t, [h_t]"
      ],
      "metadata": {
        "id": "hpZSp4u63Zuk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MUL GRU (tensorflow)"
      ],
      "metadata": {
        "id": "ofsHsKJC3iJ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiplicativeGRUCell(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    A custom implementation of the multiplicative GRU cell using TensorFlow.\n",
        "\n",
        "    Equations:\n",
        "        m_t = W_m * x_t + U_m * h_{t-1} + b_m\n",
        "        x_tilde = m_t ⊙ x_t\n",
        "\n",
        "        z_t = sigma(W_z * x_tilde + U_z * h_{t-1} + b_z)\n",
        "        r_t = sigma(W_r * x_tilde + U_r * h_{t-1} + b_r)\n",
        "        h_tilde = tanh(W_h * x_tilde + U_h * (r_t ⊙ h_{t-1}) + b_h)\n",
        "        h_t = (1 - z_t) ⊙ h_{t-1} + z_t ⊙ h_tilde\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, units, **kwargs):\n",
        "        super(MultiplicativeGRUCell, self).__init__(**kwargs)\n",
        "        self.units = units\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        \"\"\"\n",
        "        input_shape is typically (batch_size, input_dim)\n",
        "        \"\"\"\n",
        "        input_dim = input_shape[-1]\n",
        "\n",
        "        # Weights for m_t\n",
        "        self.Wm = self.add_weight( shape=(input_dim, self.units), initializer='glorot_uniform', name='Wm' )\n",
        "        self.Um = self.add_weight( shape=(self.units, self.units), initializer='orthogonal', name='Um' )\n",
        "        self.bm = self.add_weight( shape=(self.units,), initializer='zeros', name='bm' )\n",
        "\n",
        "        # Weights for update gate z\n",
        "        self.Wz = self.add_weight( shape=(self.units, self.units), initializer='glorot_uniform', name='Wz' )\n",
        "        self.Uz = self.add_weight( shape=(self.units, self.units), initializer='orthogonal', name='Uz' )\n",
        "        self.bz = self.add_weight( shape=(self.units,), initializer='zeros', name='bz' )\n",
        "\n",
        "        # Weights for reset gate r\n",
        "        self.Wr = self.add_weight( shape=(self.units, self.units), initializer='glorot_uniform', name='Wr' )\n",
        "        self.Ur = self.add_weight( shape=(self.units, self.units), initializer='orthogonal', name='Ur' )\n",
        "        self.br = self.add_weight( shape=(self.units,), initializer='zeros', name='br' )\n",
        "\n",
        "        # Weights for candidate hidden state\n",
        "        self.Wh = self.add_weight( shape=(self.units, self.units), initializer='glorot_uniform', name='Wh' )\n",
        "        self.Uh = self.add_weight( shape=(self.units, self.units), initializer='orthogonal', name='Uh' )\n",
        "        self.bh = self.add_weight( shape=(self.units,), initializer='zeros', name='bh' )\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "\n",
        "        super(MultiplicativeGRUCell, self).build(input_shape)\n",
        "\n",
        "    def call(self, inputs, states):\n",
        "        \"\"\"\n",
        "        inputs: (batch_size, input_dim)\n",
        "        states: list with a single element h_{t-1} of shape (batch_size, units)\n",
        "        \"\"\"\n",
        "        h_prev = states[0]  # shape: (batch_size, units)\n",
        "\n",
        "        # 1) Compute m_t of shape (batch_size, units)\n",
        "        #    m_t = W_m x_t + U_m h_{t-1} + b_m\n",
        "        m_t = (\n",
        "            tf.matmul(inputs, self.Wm) +\n",
        "            tf.matmul(h_prev, self.Um) +\n",
        "            self.bm\n",
        "        )  # shape: (batch_size, units)\n",
        "\n",
        "        # 2) x_tilde = m_t ⊙ x_t\n",
        "        #    But we have a shape mismatch: m_t is (batch_size, units),\n",
        "        #    while x_t is (batch_size, input_dim).\n",
        "        #\n",
        "        #    Typically, \"multiplicative\" gating is done elementwise,\n",
        "        #    so we need them to be the same dimension.\n",
        "        #    If input_dim == units, then we can do elementwise multiplication.\n",
        "        #\n",
        "        #    If they differ, you might do a diagonal approach or another transform.\n",
        "        #\n",
        "        #    For simplicity, let's assume input_dim == units for demonstration.\n",
        "        #    If your input_dim != units, you need a consistent design (like a diagonal of size input_dim).\n",
        "        #\n",
        "        #    Let's assume input_dim == units for this example.\n",
        "        x_tilde = m_t * inputs  # elementwise multiplication\n",
        "        # 3) Update gate z_t\n",
        "        z_t = tf.sigmoid( tf.matmul(x_tilde, self.Wz) + tf.matmul(h_prev, self.Uz) + self.bz )\n",
        "        # 4) Reset gate r_t\n",
        "        r_t = tf.sigmoid( tf.matmul(x_tilde, self.Wr) + tf.matmul(h_prev, self.Ur) + self.br )\n",
        "        # 5) Candidate hidden state h_tilde\n",
        "        h_tilde = tf.tanh( tf.matmul(x_tilde, self.Wh) + tf.matmul(r_t * h_prev, self.Uh) + self.bh )\n",
        "        # 6) Final new hidden state h_t\n",
        "        h_t = (1 - z_t) * h_prev + z_t * h_tilde\n",
        "        return h_t, [h_t]"
      ],
      "metadata": {
        "id": "bcReM-c03lMi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}